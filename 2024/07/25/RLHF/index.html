<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"example.com",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",width:320,display:"always",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="强化学习人类反馈（Reinforcement Learning from Human Feedback, RLHF）是一种通过结合强化学习和人类反馈来优化模型性能的方法。RLHF特别适用于那些主观性强、没有明确目标的任务，例如生成文本摘要、回答开放性问题等。"><meta property="og:type" content="article"><meta property="og:title" content="RLHF 强化学习人类反馈"><meta property="og:url" content="http://example.com/2024/07/25/RLHF/index.html"><meta property="og:site_name" content="凤凰院魔法使のBlog"><meta property="og:description" content="强化学习人类反馈（Reinforcement Learning from Human Feedback, RLHF）是一种通过结合强化学习和人类反馈来优化模型性能的方法。RLHF特别适用于那些主观性强、没有明确目标的任务，例如生成文本摘要、回答开放性问题等。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://phoenizard-picgo.oss-cn-hangzhou.aliyuncs.com/img/RLHF_1.png"><meta property="og:image" content="https://phoenizard-picgo.oss-cn-hangzhou.aliyuncs.com/img/RLHF_2.png"><meta property="og:image" content="https://phoenizard-picgo.oss-cn-hangzhou.aliyuncs.com/img/RLHF_3.png"><meta property="og:image" content="https://phoenizard-picgo.oss-cn-hangzhou.aliyuncs.com/img/RLHF_4.png"><meta property="og:image" content="https://phoenizard-picgo.oss-cn-hangzhou.aliyuncs.com/img/RLHF_5.png"><meta property="article:published_time" content="2024-07-25T02:48:50.000Z"><meta property="article:modified_time" content="2024-07-25T03:12:03.901Z"><meta property="article:author" content="Phoenizard"><meta property="article:tag" content="强化学习"><meta property="article:tag" content="LLM"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://phoenizard-picgo.oss-cn-hangzhou.aliyuncs.com/img/RLHF_1.png"><link rel="canonical" href="http://example.com/2024/07/25/RLHF/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>RLHF 强化学习人类反馈 | 凤凰院魔法使のBlog</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">凤凰院魔法使のBlog</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">34</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">8</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">30</span></a></li><li class="menu-item menu-item-音游记录"><a href="/rthymgs/" rel="section"><i class="fa fa-database fa-fw"></i>音游记录</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><a href="https://github.com/Phoenizard" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://example.com/2024/07/25/RLHF/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Phoenizard"><meta itemprop="description" content="And in that light, I find deliverance"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="凤凰院魔法使のBlog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">RLHF 强化学习人类反馈</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2024-07-25 10:48:50 / 修改时间：11:12:03" itemprop="dateCreated datePublished" datetime="2024-07-25T10:48:50+08:00">2024-07-25</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>7.4k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>7 分钟</span></span><div class="post-description">强化学习人类反馈（Reinforcement Learning from Human Feedback, RLHF）是一种通过结合强化学习和人类反馈来优化模型性能的方法。RLHF特别适用于那些主观性强、没有明确目标的任务，例如生成文本摘要、回答开放性问题等。</div></div></header><div class="post-body" itemprop="articleBody"><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>强化学习人类反馈（Reinforcement Learning from Human Feedback, RLHF）是一种通过结合强化学习和人类反馈来优化模型性能的方法。RLHF特别适用于那些<strong>主观性强、没有明确目标的任务</strong>，例如生成文本摘要、回答开放性问题等。</p><p><img src="https://phoenizard-picgo.oss-cn-hangzhou.aliyuncs.com/img/RLHF_1.png" alt=""></p><h3 id="RLHF-Process"><a href="#RLHF-Process" class="headerlink" title="RLHF Process"></a>RLHF Process</h3><h4 id="Step-1-创建偏好数据集（Preference-Dataset）"><a href="#Step-1-创建偏好数据集（Preference-Dataset）" class="headerlink" title="Step 1: 创建偏好数据集（Preference Dataset）"></a>Step 1: 创建偏好数据集（Preference Dataset）</h4><ul><li>给LLM（大型语言模型）一个文本，让基座模型生成两个摘要（Summary1和Summary2）。</li><li>人工标注：对于特定输入，给出两个摘要，并标注哪个更好。注意这里不能使用标量（scaler）评分，而是记录哪个摘要更好，即偏好数据集（Preference Dataset）。</li></ul><blockquote><p><strong>监督微调（Supervised Fine Tuning）</strong></p><ul><li>输入：文本 → 输出：摘要</li></ul><p><strong>强化学习人类反馈（RLHF）</strong></p><ul><li>输入：文本 → 输出：摘要1，摘要2，人类偏好</li></ul></blockquote><h4 id="Step-2-奖励模型（Reward-Model）"><a href="#Step-2-奖励模型（Reward-Model）" class="headerlink" title="Step 2: 奖励模型（Reward Model）"></a>Step 2: 奖励模型（Reward Model）</h4><p><img src="https://phoenizard-picgo.oss-cn-hangzhou.aliyuncs.com/img/RLHF_2.png" style="zoom:50%"></p><ul><li>{Prompt, Completion} → 奖励模型（Reward Model）（推理）→ 标量（Scaler），表示这个完成度如何</li><li>奖励模型的推理本质上是一个回归任务，其训练目标是：<ul><li>{Prompt, Winning Candidate, Losing Candidate}</li><li>目标是最大化胜出摘要与失败摘要之间的评分差异</li></ul></li></ul><h4 id="Step-3-在RL循环中使用奖励模型微调LLM"><a href="#Step-3-在RL循环中使用奖励模型微调LLM" class="headerlink" title="Step 3: 在RL循环中使用奖励模型微调LLM"></a>Step 3: 在RL循环中使用奖励模型微调LLM</h4><ul><li><p>调整基座模型的输出以最大化奖励。</p></li><li><p>引入一个新的提示数据集（Prompt Dataset），仅为提示数据集。</p><blockquote><p>强化学习（Reinforcement Learning）适用于复杂但开放的任务训练方法，Agent通过与环境的交互来解决任务。Agent通过行为作用在环境中，环境更新Agent的状态并给出奖励（正向或负向）。通过重复这个过程，Agent使用一个函数记录学习结果，函数以当前状态为输入，输出最合适的行为（Policy）。</p></blockquote></li></ul><h5 id="RLHF在LLM中的应用"><a href="#RLHF在LLM中的应用" class="headerlink" title="RLHF在LLM中的应用"></a>RLHF在LLM中的应用</h5><ul><li><strong>Agent</strong>：初始策略为基座LLM生成策略，初始状态为文本信息（Prompt）。</li><li>每次LLM生成tokens完成输出后，都会从奖励模型得到反馈分数：<ul><li>{Prompt, Completion} → Score</li></ul></li><li>学习目标是获得最高分数的生成策略（Policy），学习方法为近端策略优化（Proximal Policy Optimization, PPO）。</li></ul><p><img src="https://phoenizard-picgo.oss-cn-hangzhou.aliyuncs.com/img/RLHF_3.png" style="zoom:50%"></p><h3 id="代码实践"><a href="#代码实践" class="headerlink" title="代码实践"></a>代码实践</h3><h4 id="微调方法"><a href="#微调方法" class="headerlink" title="微调方法"></a>微调方法</h4><ol><li><strong>完全微调（Full Fine Tuning）</strong>：更新所有权重。</li><li><strong>参数高效微调（Parameter Efficient Tuning）</strong>：只更新一部分参数，保留基座模型中的其他参数。</li></ol><h4 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h4><ol><li><p>偏好数据集：用于训练奖励模型</p><p>字典Key为<code>[&#39;input_text&#39;, &#39;candidate_0&#39;, &#39;candidate_1&#39;, &#39;choice&#39;]</code></p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">&#x27;input_text&#x27;<span class="punctuation">:</span> &#x27;I live right next to a huge university<span class="punctuation">,</span> and have been applying for a variety of jobs with them through ... <span class="punctuation">[</span>summary<span class="punctuation">]</span><span class="punctuation">:</span> &#x27;<span class="punctuation">,</span> </span><br><span class="line">&#x27;candidate_0&#x27;<span class="punctuation">:</span> &#x27; When applying through a massive job portal<span class="punctuation">,</span> is just one HR person seeing ALL of them?&#x27;<span class="punctuation">,</span> </span><br><span class="line">&#x27;candidate_1&#x27;<span class="punctuation">:</span> &#x27; When applying to many jobs through a single university jobs portal<span class="punctuation">,</span> is just one HR person reading ALL my applications?&#x27;<span class="punctuation">,</span> </span><br><span class="line">&#x27;choice&#x27;<span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>我们可以看到input text末尾有<code>[summary]:</code> 标记，用于标记任务为summary任务</p></li><li><p>Prompt数据集: 用于强化学习循环，在演示中仅有6条: 每个元素的格式为key+value:</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span>&#x27;input_text&#x27;<span class="punctuation">:</span> <span class="string">&quot;Nooooooo, I loved my health class! My teacher was amazing! Most days we just went outside and played and the facility allowed it because the health teacher&#x27;s argument was that teens need to spend time outside everyday and he let us do that. The other days were spent inside with him teaching us how to live a healthy lifestyle. He had guest speakers come in and reach us about nutrition and our final was open book...if we even had a final.... [summary]: &quot;</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="RLHF-Pipeline"><a href="#RLHF-Pipeline" class="headerlink" title="RLHF Pipeline"></a>RLHF Pipeline</h3><p>RLHF Pipeline就是对下图流程的复现：</p><p><img src="https://phoenizard-picgo.oss-cn-hangzhou.aliyuncs.com/img/RLHF_4.png" style="zoom:33%"></p><p>在实例中使用了谷歌云Pipeline平台，Pipeline由yaml文件定义，是一个完整了RLHF流程，我们需要导入平台并且准备好数据的路径。训练使用Llama2作为基座模型。</p><h4 id="奖励模型训练步数设置"><a href="#奖励模型训练步数设置" class="headerlink" title="奖励模型训练步数设置"></a>奖励模型训练步数设置</h4><p><strong>Reward_model_train_steps</strong>是训练奖励模型时使用的步数。这取决于首选项数据集的大小。我们建议模型应该在偏好数据集上训练20-30个epoch以获得最佳结果。</p><script type="math/tex;mode=display">\begin{align*} \text{stepsPerEpoch} &= \left\lceil \frac{\text{datasetSize}}{\text{batchSize}} \right\rceil \\ \text{trainSteps} &= \text{stepsPerEpoch} \times \text{numEpochs} \end{align*}</script><p>RLHF管道参数要求的是训练步数，而不是epoch数。下面是如何从epoch到训练步骤的示例，假设此管道的批处理大小固定为每批64个示例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练数据大小</span></span><br><span class="line">PREF_DATASET_SIZE = <span class="number">3000</span></span><br><span class="line"><span class="comment"># 批次大小</span></span><br><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">REWARD_STEPS_PER_EPOCH = math.ceil(PREF_DATASET_SIZE / BATCH_SIZE) <span class="comment"># 约47次</span></span><br><span class="line"></span><br><span class="line">REWARD_NUM_EPOCHS = <span class="number">30</span> 期待的迭代次数</span><br><span class="line"><span class="comment"># 计算训练步数</span></span><br><span class="line">reward_model_train_steps = REWARD_STEPS_PER_EPOCH * REWARD_NUM_EPOCHS <span class="comment"># 1410</span></span><br></pre></td></tr></table></figure><h4 id="强化模型训练步数设置"><a href="#强化模型训练步数设置" class="headerlink" title="强化模型训练步数设置"></a>强化模型训练步数设置</h4><p><strong>Reinforcement learning train steps</strong> 参数是指在调优基础模型时要执行的强化学习步数。</p><ul><li>训练步数取决于提示数据集的大小。通常，这个模型应该在提示数据集上训练大约10-20个epoch。</li><li>Reward hacking：如果给予过多的训练步数，策略模型可能会找到一种方法来利用奖励，从而表现出不期望的行为。</li></ul><p>强化模型的Epoch数通常为10-20，我们根据这个epoch计算总共的训练次数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Prompt dataset size</span></span><br><span class="line">PROMPT_DATASET_SIZE = <span class="number">2000</span></span><br><span class="line"><span class="comment"># Batch size is fixed at 64</span></span><br><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">RL_STEPS_PER_EPOCH = math.ceil(PROMPT_DATASET_SIZE / BATCH_SIZE)</span><br><span class="line"><span class="built_in">print</span>(RL_STEPS_PER_EPOCH)</span><br><span class="line"></span><br><span class="line">RL_NUM_EPOCHS = <span class="number">10</span></span><br><span class="line"><span class="comment"># Calculate the number of steps in the RL training</span></span><br><span class="line">reinforcement_learning_train_steps = RL_STEPS_PER_EPOCH * RL_NUM_EPOCHS <span class="comment"># 320</span></span><br></pre></td></tr></table></figure><h4 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h4><p>根据上面计算的步长结果，填写整个任务流程中的超参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Completed values for the dictionary</span></span><br><span class="line">parameter_values=&#123;</span><br><span class="line">        <span class="string">&quot;preference_dataset&quot;</span>: \\</span><br><span class="line">    <span class="string">&quot;gs://vertex-ai/generative-ai/rlhf/text_small/summarize_from_feedback_tfds/comparisons/train/*.jsonl&quot;</span>,</span><br><span class="line">        <span class="string">&quot;prompt_dataset&quot;</span>: \\</span><br><span class="line">    <span class="string">&quot;gs://vertex-ai/generative-ai/rlhf/text_small/reddit_tfds/train/*.jsonl&quot;</span>,</span><br><span class="line">        <span class="string">&quot;eval_dataset&quot;</span>: \\</span><br><span class="line">    <span class="string">&quot;gs://vertex-ai/generative-ai/rlhf/text_small/reddit_tfds/val/*.jsonl&quot;</span>,</span><br><span class="line">    <span class="comment"># 数据集地址</span></span><br><span class="line">        <span class="string">&quot;large_model_reference&quot;</span>: <span class="string">&quot;llama-2-7b&quot;</span>, <span class="comment"># 基座模型</span></span><br><span class="line">        <span class="string">&quot;reward_model_train_steps&quot;</span>: <span class="number">1410</span>, </span><br><span class="line">        <span class="string">&quot;reinforcement_learning_train_steps&quot;</span>: <span class="number">320</span>, <span class="comment"># results from the calculations above</span></span><br><span class="line">        <span class="string">&quot;reward_model_learning_rate_multiplier&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">        <span class="string">&quot;reinforcement_learning_rate_multiplier&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">        <span class="string">&quot;kl_coeff&quot;</span>: <span class="number">0.1</span>, <span class="comment"># increased to reduce reward hacking</span></span><br><span class="line">        <span class="string">&quot;instruction&quot;</span>:\\</span><br><span class="line">    <span class="string">&quot;Summarize in less than 50 words&quot;</span> <span class="comment"># 补充的提示词&#125;</span></span><br></pre></td></tr></table></figure><h4 id="训练结果"><a href="#训练结果" class="headerlink" title="训练结果"></a>训练结果</h4><ul><li>训练奖励模型损失函数: 收敛较好</li></ul><p><img src="https://phoenizard-picgo.oss-cn-hangzhou.aliyuncs.com/img/RLHF_5.png" style="zoom:40%"></p><ul><li><p>PPO训练曲线</p><ul><li><p>KL Loss: KL (Kullback-Leibler) 散度（KL divergence）是一种衡量两个概率分布之间差异的非对称度量。在强化学习中，KL 散度通常用于衡量新策略与旧策略之间的差异，以确保策略更新不会过于激进，从而保持训练的稳定性。</p><blockquote><p><strong>初期快速上升</strong>：这是比较常见的现象，说明模型在初期调整过程中，策略变化较大，从而导致与旧策略之间的差异增大。</p><p><strong>波动期</strong>：这段时间的波动可能是由于强化学习过程中的探索与利用权衡，模型在尝试不同策略以寻找最优解。这种波动也可能表明模型在调整过程中遇到了一些不稳定性。</p><p><strong>稳定期</strong>：在训练的后期，KL损失趋于稳定，表明模型的策略更新幅度减小，策略逐渐稳定。这也说明模型已经找到了一个相对稳定的策略，不再进行大的调整。</p></blockquote></li></ul></li><li><p>两个Examples</p></li></ul><div class="table-container"><table><thead><tr><th>Input Text</th><th>Base LLM</th><th>Tuned LLM</th></tr></thead><tbody><tr><td>Summarize in less than 50 words. Before anything, not a sad story or anything. My country’s equivalent to Valentine’s Day is coming and I had this pretty simple idea to surprise my girlfriend and it would involve giving her some roses. The thing is, although I know she would appreciate my intention in and of itself, I don’t know if she would like the actual flowers and such, so I wanted to find out if she likes roses and if she would like getting some, but without her realizing it so as not to spoil the surprise. Any ideas on how to get that information out of her? [summary]:</td><td>The author wants to surprise his girlfriend with roses on Valentine’s Day but he doesn’t know if she likes roses. He wants to find out without spoiling the surprise.</td><td>My country’s equivalent to Valentine’s Day is coming. Want to surprise my girlfriend with roses but don’t know if she would like getting some. Any ideas on how to get that information out of her without spoiling the surprise</td></tr><tr><td>Summarize in less than 50 words. Hello jobs, I graduated a few months ago and had no luck so far to get a job in marketing/sales. There’s a small local company (perhaps 30 employees) but they are actually pretty successful in what they’re doing (known worldwide). I checked their website and it’s awful. Looks like a website from the early 2000’s. So I guess they are not pretty good in (online-)marketing. I would like to do a cold application (not sure if they are looking for a marketing guy) but I had no luck with this kind of application in the past. That’s why I thought I try something different. I have good skills in Photoshop, InDesign and Illustrator. As a teenager I also built websites using HTML, so I thought I build a dummy website fitted to their company and attach some screenshots to my application. What do you think? I this a feasible idea or will they be offended? Thank you very much in advance. [summary]:</td><td>Graduated a few months ago, looking for a job in marketing/sales. Found a small local company that is successful but has an awful website. I want to apply for a marketing job but not sure if they are hiring. I thought of building a dummy website for them and attach some screenshots to my application.</td><td>Graduated a few months ago. No luck so far to get a job in marketing/sales. Thought to build a dummy website fitted to the company and attach some screenshots to my application. What do you think?</td></tr></tbody></table></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"># 强化学习</a> <a href="/tags/LLM/" rel="tag"># LLM</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2024/05/08/Phase%20Plane%20Methods/" rel="prev" title="Phase Plane Method"><i class="fa fa-chevron-left"></i> Phase Plane Method</a></div><div class="post-nav-item"><a href="/2024/07/25/tensorboard/" rel="next" title="Tensorboard 部署测试">Tensorboard 部署测试 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RLHF-Process"><span class="nav-number">2.</span> <span class="nav-text">RLHF Process</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Step-1-%E5%88%9B%E5%BB%BA%E5%81%8F%E5%A5%BD%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%88Preference-Dataset%EF%BC%89"><span class="nav-number">2.1.</span> <span class="nav-text">Step 1: 创建偏好数据集（Preference Dataset）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Step-2-%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B%EF%BC%88Reward-Model%EF%BC%89"><span class="nav-number">2.2.</span> <span class="nav-text">Step 2: 奖励模型（Reward Model）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Step-3-%E5%9C%A8RL%E5%BE%AA%E7%8E%AF%E4%B8%AD%E4%BD%BF%E7%94%A8%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83LLM"><span class="nav-number">2.3.</span> <span class="nav-text">Step 3: 在RL循环中使用奖励模型微调LLM</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#RLHF%E5%9C%A8LLM%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="nav-number">2.3.1.</span> <span class="nav-text">RLHF在LLM中的应用</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5"><span class="nav-number">3.</span> <span class="nav-text">代码实践</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95"><span class="nav-number">3.1.</span> <span class="nav-text">微调方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="nav-number">3.2.</span> <span class="nav-text">数据准备</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RLHF-Pipeline"><span class="nav-number">4.</span> <span class="nav-text">RLHF Pipeline</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%AD%A5%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="nav-number">4.1.</span> <span class="nav-text">奖励模型训练步数设置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%AD%A5%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="nav-number">4.2.</span> <span class="nav-text">强化模型训练步数设置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="nav-number">4.3.</span> <span class="nav-text">参数设置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C"><span class="nav-number">4.4.</span> <span class="nav-text">训练结果</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Phoenizard" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">Phoenizard</p><div class="site-description" itemprop="description">And in that light, I find deliverance</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">30</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">8</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">34</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/Phoenizard" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Phoenizard" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:jiangxiaoyi2004@163.com" title="E-Mail → mailto:jiangxiaoyi2004@163.com" rel="noopener" target="_blank"><i class="fab fa-envelope fa-fw"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="https://profile-pjsekai.kirafan.cn/#/user/196266691860643843" title="PJSKProfile → https:&#x2F;&#x2F;profile-pjsekai.kirafan.cn&#x2F;#&#x2F;user&#x2F;196266691860643843" rel="noopener" target="_blank"><i class="fa-fw"></i>PJSKProfile</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Phoenizard</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span title="站点总字数">109k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">1:39</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动</div><div><span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><script>var now=new Date;function createtime(){var n=new Date("08/24/2022 00:00:00");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="本站已安全运行 "+dnum+" 天 ",document.getElementById("times").innerHTML=hnum+" 小时 "+mnum+" 分 "+snum+" 秒"}setInterval("createtime()",250)</script></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script></body></html>